{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Notation\n",
    "\n",
    "m: number of examples\n",
    "\n",
    "$n_x$: number of variables(features)\n",
    "\n",
    "$n_y$: number of output(classes) \n",
    "\n",
    "x.shape: (n_x, m)\n",
    "\n",
    "y.shape: (n_y, m)\n",
    "\n",
    "W: (m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Initialize parameters\n",
    "- Weights and biases are parameters that model connections between different layers\n",
    "- Define parameter shape: keep the implementation as close as possible to the mathmatical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(outs, ins):\n",
    "    return np.random.default_rng().normal(loc=0, scale=1/(outs * ins), size=(outs, ins))\n",
    "    # return np.zeros((outs, ins))\n",
    "\n",
    "def initialize_bias(outs):\n",
    "    \"\"\"create a column vector as a matrix\"\"\"\n",
    "    # return np.zeros((outs, 1))\n",
    "    return initialize_weights(outs, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 2\n",
      "cols: 4\n",
      "\n",
      "weight W:\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "W.shape: (2, 4)\n",
      "\n",
      "bias b:\n",
      " [[0.]\n",
      " [0.]]\n",
      "b.shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "nrows = 2 # output number\n",
    "ncols = 4 # input number\n",
    "\n",
    "W = initialize_weights(nrows, ncols)\n",
    "b = initialize_bias(nrows)\n",
    "\n",
    "print(\"rows:\", nrows)\n",
    "print(\"cols:\", ncols)\n",
    "print()\n",
    "print(\"weight W:\\n\", W)\n",
    "print(\"W.shape:\", W.shape)\n",
    "print()\n",
    "print(\"bias b:\\n\", b)\n",
    "print(\"b.shape:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Update neuron states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-1. Linear Model; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x:\n",
      " [[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "x.shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array((1, 0, 0, 0), ndmin=2).T\n",
    "\n",
    "print(\"input x:\\n\", x)\n",
    "print(\"x.shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z :\n",
      " [[0.]\n",
      " [0.]]\n",
      "z.shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "z = np.dot(W, x) + b\n",
    "\n",
    "print(\"z :\\n\", z)\n",
    "print(\"z.shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2-2. Activation function; a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, leaky_param=0.1):\n",
    "    return np.maximum(x, x * leaky_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " [[0.]\n",
      " [0.]]\n",
      "a.shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "a = leaky_relu(z)\n",
    "\n",
    "print(\"a:\\n\", a)\n",
    "print(\"a.shape:\", a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Modelling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer class that represents the connections and the flow of information between a column of neurons and the next.\n",
    "    It deals with what happens in between two columns of neurons instead of having the layer specifially represent the neurons of each vertical column\n",
    "    \"\"\"\n",
    "    def __init__(self, ins, outs, act_function) -> None:\n",
    "        self.ins = ins\n",
    "        self.outs = outs\n",
    "        self.act_function = act_function\n",
    "\n",
    "        self._W = initialize_weights(self.outs, self.ins)\n",
    "        self._b = initialize_bias(self.outs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        helper method that computes the forward pass in the layer\n",
    "\n",
    "        Parameters:\n",
    "        x: a set of neuron states\n",
    "\n",
    "        Returns:\n",
    "        the next set of neuron states\n",
    "        \"\"\"\n",
    "        return self.act_function(np.dot(self._W, x) + self._b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Layer chain demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[0.08229661 0.2097671  0.15119465 0.17063379 0.98812121]\n",
      " [0.93262093 0.70900699 0.49369313 0.86071326 0.7889657 ]]\n",
      "x.shape: (2, 5)\n",
      "\n",
      "output:\n",
      " [[0. 0. 0. 0. 0.]] (1, 5)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Demo of chaining layers with compatible shapes.\n",
    "    \"\"\"\n",
    "    input_shape = 2 # n_x\n",
    "    output_shape = 1 # n_y\n",
    "\n",
    "    l1 = Layer(input_shape, 4, leaky_relu)\n",
    "    l2 = Layer(4, 4, leaky_relu)\n",
    "    l3 = Layer(4, output_shape, leaky_relu)\n",
    "\n",
    "    # x.shape (n_x, m)\n",
    "    x = np.random.uniform(size=(input_shape, 5))\n",
    "    print(\"x:\\n\", x)\n",
    "    print(\"x.shape:\", x.shape)\n",
    "\n",
    "    # y.shape (n_y, m)\n",
    "    y = l3.forward(l2.forward(l1.forward(x)))\n",
    "    print(\"\\noutput:\\n\", y, y.shape)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    A series of layers connected and compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers) -> None:\n",
    "        self._layers = layers\n",
    "        self.check_layer_compatibility()\n",
    "\n",
    "    def check_layer_compatibility(self):\n",
    "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
    "            print(\"from, to:\", from_.ins, to_.ins)\n",
    "            if from_.outs != to_.ins:\n",
    "                raise ValueError(\"Layers should have compatible shapes.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self._layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return self._loss_function(y_pred, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Neural Net demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from, to: 2 4\n",
      "from, to: 4 4\n",
      "x:\n",
      " [[0.73704196 0.72135886 0.69032522]\n",
      " [0.99822564 0.25076019 0.62377661]]\n",
      "x.shape: (2, 3)\n",
      "\n",
      "y:\n",
      " [[0. 0. 0.]] (1, 3)\n"
     ]
    }
   ],
   "source": [
    "def nn_demo():\n",
    "    \"\"\"\n",
    "    Demo of a network as a serias of layers.\n",
    "    \"\"\"\n",
    "    input_shape = 2 # n_x\n",
    "    output_shape = 1 # n_y\n",
    "\n",
    "    net = NeuralNet(\n",
    "        [\n",
    "            Layer(input_shape, 4, leaky_relu),\n",
    "            Layer(4, 4, leaky_relu),\n",
    "            Layer(4, output_shape, leaky_relu),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # x (n_x, m) \n",
    "    x = np.random.uniform(size=(input_shape, 3))\n",
    "    print(\"x:\\n\", x)\n",
    "    print(\"x.shape:\", x.shape)\n",
    "\n",
    "    # y (n_y, m)\n",
    "    y = net.forward(x) \n",
    "    print(\"\\ny:\\n\", y, y.shape)\n",
    "    \n",
    "nn_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer compatibility test\n",
    "net = NeuralNet(\n",
    "    [\n",
    "        Layer(2, 4, leaky_relu),\n",
    "        Layer(5, 1, leaky_relu),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Evaluate performance - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    return np.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    A series of layers connected and compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_function) -> None:\n",
    "        self._layers = layers\n",
    "        self._loss_function = loss_function\n",
    "        self.check_layer_compatibility()\n",
    "\n",
    "    def check_layer_compatibility(self):\n",
    "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
    "            print(\"from, to:\", from_.ins, to_.ins)\n",
    "            if from_.outs != to_.ins:\n",
    "                raise ValueError(\"Layers should have compatible shapes.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self._layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return self._loss_function(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-1. Derivative of activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_leaky_relu(x, leaky_param=0.1):\n",
    "    return np.maximum(x > 0, leaky_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 1. , 1. ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-2, -1, 3, 4])\n",
    "x\n",
    "d_leaky_relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-2. Derivative of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_derivative(y_pred, y):\n",
    "    return 2 * (y_pred - y) / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. , 1.5, 2. ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "x\n",
    "t = np.zeros(shape=4)\n",
    "t\n",
    "MSE_derivative(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. Generic Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4-1. Generic Activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"\n",
    "    Class to be inherited by activation functions.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def f(self, x):\n",
    "        \"\"\"\n",
    "        Method that implements the function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def df(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of the function with respect to its input.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRelu(Activation):\n",
    "    \"\"\"\n",
    "    Leaky Rectified Linear Unit.\n",
    "    \"\"\"\n",
    "    def __init__(self, leaky_param=0.1):\n",
    "        self.alpha = leaky_param\n",
    "\n",
    "    def f(self, x):\n",
    "        return np.maximum(x, x * self.alpha)\n",
    "\n",
    "    def df(self, x):\n",
    "        return np.maximum(x > 0, self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4-2. Generic Loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Class to be inherited by loss functions.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def loss(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def dloss(self, y_pred, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def loss(self, y_pred, y):\n",
    "        return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "    def dloss(self, y_pred, y):\n",
    "        return 2 * (y_pred - y) / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-5. Update Layer and NeuralNet classes acorrding to Activation and Loss classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer class that represents the connections and the flow of information between a column of neurons and the next.\n",
    "    It deals with what happens in between two columns of neurons instead of having the layer specifially represent the neurons of each vertical column\n",
    "    \"\"\"\n",
    "    def __init__(self, ins, outs, act_function) -> None:\n",
    "        self.ins = ins\n",
    "        self.outs = outs\n",
    "        self.act_function = act_function\n",
    "\n",
    "        self._W = initialize_weights(self.outs, self.ins)\n",
    "        self._b = initialize_bias(self.outs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        helper method that computes the forward pass in the layer\n",
    "\n",
    "        Parameters:\n",
    "        x: a set of neuron states\n",
    "\n",
    "        Returns:\n",
    "        the next set of neuron states\n",
    "        \"\"\"\n",
    "        return self.act_function.f(np.dot(self._W, x) + self._b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    A series of layers connected and compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_function) -> None:\n",
    "        self._layers = layers\n",
    "        self._loss_function = loss_function\n",
    "        self.check_layer_compatibility()\n",
    "\n",
    "    def check_layer_compatibility(self):\n",
    "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
    "            print(\"from, to:\", from_.ins, to_.ins)\n",
    "            if from_.outs != to_.ins:\n",
    "                raise ValueError(\"Layers should have compatible shapes.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self._layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return self._loss_function.loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. Generic NN demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from, to: 2 4\n",
      "from, to: 4 4\n",
      "x:\n",
      " [[0.60080465]\n",
      " [0.44181555]]\n",
      "x.shape: (2, 1)\n",
      "\n",
      "y:\n",
      " [[0.]]\n",
      "loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "def generic_nn_demo():\n",
    "    \"\"\"\n",
    "    Demo of a network as a serias of layers.\n",
    "    \"\"\"\n",
    "    input_shape = 2 # n_x\n",
    "    output_shape = 1 # n_y\n",
    "    net = NeuralNet(\n",
    "        [\n",
    "            Layer(input_shape, 4, LeakyRelu()),\n",
    "            Layer(4, 4, LeakyRelu()),\n",
    "            Layer(4, output_shape, LeakyRelu()),\n",
    "        ],\n",
    "        MSE()\n",
    "    )\n",
    "\n",
    "    # x\n",
    "    x = np.random.uniform(size=(input_shape, 1))\n",
    "    print(\"x:\\n\", x)\n",
    "    print(\"x.shape:\", x.shape)\n",
    "\n",
    "    # y \n",
    "    y = net.forward(x) \n",
    "    print(\"\\ny:\\n\", y)\n",
    "    print(\"loss: \", net.loss(y, np.array(0, ndmin=2)))\n",
    "\n",
    "generic_nn_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    A series of layers connected and compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_function) -> None:\n",
    "        self._layers = layers\n",
    "        self._loss_function = loss_function\n",
    "        self.check_layer_compatibility()\n",
    "\n",
    "    def check_layer_compatibility(self):\n",
    "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
    "            print(\"from, to:\", from_.ins, to_.ins)\n",
    "            if from_.outs != to_.ins:\n",
    "                raise ValueError(\"Layers should have compatible shapes.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [x]\n",
    "        for layer in self._layers:\n",
    "            xs.append(layer.forward(xs[-1]))\n",
    "        return xs\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return self._loss_function.loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. The chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfold the recursive definition of $x_n$:\n",
    "$$L(x_n, t) = L(f_{n - 1}(W_{n - 1} \\cdot x_{n - 1} + b_{n - 1}), t)$$\n",
    "$$= L(f_{n - 1}(W_{n - 1} \\cdot f_{n - 2}(W_{n - 2} \\cdot x_{n - 2} + b_{n - 2}) + b_{n - 1}), t) \\\\ = \\dots $$\n",
    "\n",
    "Taking the partial derivative of $L$ with respect to $W_0$ will involve many more chain rules than with respect to $W_{n - 1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from, to: 2 4\n",
      "from, to: 4 4\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    [\n",
    "        Layer(2, 4, LeakyRelu()),\n",
    "        Layer(4, 4, LeakyRelu()),\n",
    "        Layer(4, 1, LeakyRelu()),\n",
    "    ],\n",
    "    MSE()\n",
    ")\n",
    "n = len(net._layers)\n",
    "\n",
    "\n",
    "Ws = [layer._W for layer in net._layers]\n",
    "bs = [layer._b for layer in net._layers]\n",
    "fs = [layer.act_function.f for layer in net._layers]\n",
    "dfs = [layer.act_function.df for layer in net._layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = net._loss_function.loss\n",
    "dL = net._loss_function.dloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-1. First step\n",
    "$$\\frac {\\partial L} {\\partial x_{n - 1}}, \\frac {\\partial L} {\\partial W_{n - 1}}, \\frac {\\partial L} {\\partial b_{n - 1}}$$\n",
    "\n",
    "$$y_{n - 1} = W_{n - 1} \\cdot x_{n - 1} + b_{n - 1}$$\n",
    "$$\\frac {\\partial L} {\\partial b_{n - 1}} = dL(x_n, t) f^{'}_{n - 1}(y_{n - 1})$$\n",
    "$$\\frac {\\partial L} {\\partial x_{n - 1}} = W^{T}_{n - 1}\\frac {\\partial L} {\\partial b_{n - 1}}$$\n",
    "$$\\frac {\\partial L} {\\partial W_{n - 1}} = \\frac {\\partial L} {\\partial b_{n - 1}}x^{T}_{n - 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = L(fs[n - 1](np.dot(Ws[n - 1], xs[n - 1]) + bs[n - 1]), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(Ws[n - 1], xs[n - 1]) + bs[n - 1]\n",
    "\n",
    "dbs[n - 1] = dfs[n - 1](y) * dL(xs[n], t)\n",
    "dxs[n - 1] = np.dot(Ws[n - 1].T, dbs[n - 1])\n",
    "dWs[n - 1] = np.dot(dbs[n - 1], xs[n - 1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1-2. Next step\n",
    "#### Loss with dependence on xs[n - 2], Ws[n - 2] and bs[n - 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial L} {\\partial x_{n - 2}}, \\frac {\\partial L} {\\partial W_{n - 2}}, \\frac {\\partial L} {\\partial b_{n - 2}}$$\n",
    "\n",
    "$$y_{n - 2} = W_{n - 2} \\cdot x_{n - 2} + b_{n - 2}$$\n",
    "$$\\frac {\\partial L} {\\partial b_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial b_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} f^{'}_{n - 2}(y_{n - 2})$$\n",
    "$$\\frac {\\partial L} {\\partial x_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial x_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} f^{'}_{n - 2}(y_{n - 2}) \\frac {\\partial y_{n - 2}} {\\partial x_{n - 2}}= W^{T}_{n - 2}\\frac {\\partial L} {\\partial b_{n - 2}}$$\n",
    "$$\\frac {\\partial L} {\\partial W_{n - 2}} = \\frac {\\partial L} {\\partial x_{n - 1}} \\frac {\\partial x_{n - 1}} {\\partial W_{n - 2}} = \\frac {\\partial L} {\\partial b_{n - 2}}x^{T}_{n - 2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = L(fs[n - 1](\n",
    "    np.dot(\n",
    "        Ws[n - 1],\n",
    "        fs[n - 2](\n",
    "            np.dot(\n",
    "                Ws[n - 2],\n",
    "                xs[n - 2]\n",
    "            ) + bs[n - 2]\n",
    "        )\n",
    "    ) + bs[n - 1]\n",
    "), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(Ws[n - 2], xs[n - 2]) + bs[n - 2]\n",
    "\n",
    "dbs[n - 2] = dfs[n - 2](y) * dxs[n - 1]\n",
    "dxs[n - 2] = np.dot(Ws[n - 2].T, dbs[n - 2])\n",
    "dWs[n - 2] = np.dot(dbs[n - 2], xs[n - 2].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The General step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(Ws[n - 3], xs[n - 3]) + bs[n - 3]\n",
    "\n",
    "dbs[n - 3] = dfs[n - 3](y) * dL(xs[n - 2], t)\n",
    "dxs[n - 3] = np.dot(Ws[n - 3].T, dbs[n - 3])\n",
    "dWs[n - 3] = np.dot(dbs[n - 3], xs[n - 3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.dot(Ws[0], xs[0]) + bs[0]\n",
    "\n",
    "dbs[0] = dfs[0](y) * dL(xs[1], t)\n",
    "dxs[0] = np.dot(Ws[0].T, dbs[0])\n",
    "dWs[0] = np.dot(dbs[0], xs[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs, dWs = [], []\n",
    "dxs = [dL(xs[n], t)]\n",
    "\n",
    "for x, W, b, f in reversed(list(zip(xs[:-1], Ws, bs, fs))):\n",
    "    y = np.dot(W, x) + b\n",
    "    dbs.append(f(y) * dxs[-1])\n",
    "    dxs.append(np.dot(W.T, dbs[-1]))\n",
    "    dWs.append(np.dot(dbs[-1], x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial L} {\\partial x_{n}} = dL(x_n, t)$$\n",
    "\n",
    "$$\\frac {\\partial L} {\\partial b_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} f^{'}_{n - i}(W_{n - i} \\cdot x_{n - i} + b_{n - i})$$\n",
    "$$\\frac {\\partial L} {\\partial x_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} \\frac {\\partial x_{n - i + 1}} {\\partial x_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} f^{'}_{n - i}(y_{n - i}) \\frac {\\partial y_{n - i}} {\\partial x_{n - i}}= W^{T}_{n - i}\\frac {\\partial L} {\\partial b_{n - i}}$$\n",
    "$$\\frac {\\partial L} {\\partial W_{n - i}} = \\frac {\\partial L} {\\partial x_{n - i + 1}} \\frac {\\partial x_{n - i + 1}} {\\partial W_{n - i}} = \\frac {\\partial L} {\\partial b_{n - i}}x^{T}_{n - i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    A series of layers connected and compatible.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_function, lr) -> None:\n",
    "        self._layers = layers\n",
    "        self._loss_function = loss_function\n",
    "        self.lr = lr\n",
    "        self.check_layer_compatibility()\n",
    "\n",
    "    def check_layer_compatibility(self):\n",
    "        for from_, to_ in zip(self._layers[:-1], self._layers[1:]):\n",
    "            print(\"from, to:\", from_.ins, to_.ins)\n",
    "            if from_.outs != to_.ins:\n",
    "                raise ValueError(\"Layers should have compatible shapes.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # xs = [x]\n",
    "        # for layer in self._layers:\n",
    "        #     xs.append(layer.forward(xs[-1]))\n",
    "        # return xs\n",
    "        out = x\n",
    "        for layer in self._layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return self._loss_function.loss(y_pred, y)\n",
    "\n",
    "    def train(self, x, t):\n",
    "        \"\"\"\n",
    "        Train the network on input x and expected output t.\n",
    "        \"\"\"\n",
    "        # Accumulate intermediate results during forward pass.\n",
    "        xs = [x]\n",
    "        for layer in self._layers:\n",
    "            xs.append(layer.forward(xs[-1]))\n",
    "\n",
    "        dx = self._loss_function.dloss(xs.pop(), t)\n",
    "        for layer, x in zip(self._layers[::-1], xs[::-1]):\n",
    "\n",
    "            # Compute the derivatives\n",
    "            y = np.dot(layer._W, x) + layer._b\n",
    "            db = layer.act_function.df(y) * dx\n",
    "            dx = np.dot(layer._W.T, db)\n",
    "            dW = np.dot(db, x.T)\n",
    "\n",
    "            # Update parameters\n",
    "            layer._W -= self.lr * dW\n",
    "            layer._b -= self.lr * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Generic Train demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from, to: 2 4\n",
      "from, to: 4 4\n",
      "y:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "loss:\n",
      " 0.0\n",
      "loss:\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "def generic_train_demo():\n",
    "    \"\"\"\n",
    "    Demo of a network as a serias of layers.\n",
    "    \"\"\"\n",
    "    input_shape = 2\n",
    "    output_shape = 3\n",
    "\n",
    "    net = NeuralNet(\n",
    "        [\n",
    "            Layer(input_shape, 4, LeakyRelu()),\n",
    "            Layer(4, 4, LeakyRelu()),\n",
    "            Layer(4, output_shape, LeakyRelu()),\n",
    "        ],\n",
    "        MSE(), 0.001\n",
    "    )\n",
    "    y = np.zeros(shape=(output_shape, 1))\n",
    "    print(\"y:\\n\", y)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    for _ in range(100):\n",
    "        x = np.random.normal(size=(input_shape, 1))\n",
    "        loss += net.loss(net.forward(x)[-1], y)\n",
    "    print(\"loss:\\n\", loss)\n",
    "\n",
    "    for _ in range(10000):\n",
    "        x = np.random.normal(size=(input_shape, 1))\n",
    "        net.train(x, y)\n",
    "\n",
    "    loss = 0\n",
    "    for _ in range(100):\n",
    "        x = np.random.normal(size=(input_shape, 1))\n",
    "        loss += net.loss(net.forward(x)[-1], y)\n",
    "    print(\"loss:\\n\", loss)\n",
    "\n",
    "generic_train_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "42AI-cjung-mo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
